{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa634269-d879-4252-98a4-ca6b8c655d5a",
   "metadata": {},
   "source": [
    "Stage 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "039e0078-29b8-40e7-870d-41f007c8743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of the data: (400, 26)\n",
      "\n",
      "--- Missing values after imputation ---\n",
      "age               0\n",
      "bp                0\n",
      "sg                0\n",
      "al                0\n",
      "su                0\n",
      "rbc               0\n",
      "pc                0\n",
      "pcc               0\n",
      "ba                0\n",
      "bgr               0\n",
      "bu                0\n",
      "sc                0\n",
      "sod               0\n",
      "pot               0\n",
      "hemo              0\n",
      "pcv               0\n",
      "wc                0\n",
      "rc                0\n",
      "htn               0\n",
      "dm                0\n",
      "cad               0\n",
      "appet             0\n",
      "pe                0\n",
      "ane               0\n",
      "classification    0\n",
      "dtype: int64\n",
      "\n",
      "--- Target variable 'classification' counts ---\n",
      "classification\n",
      "1    248\n",
      "0    150\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Preprocessing Complete ---\n",
      "Processed Features (X_processed) head:\n",
      "        age        bp    sg   al   su  rbc   pc  pcc   ba       bgr  ...  \\\n",
      "0  0.522727  0.230769  0.75  0.2  0.0  1.0  1.0  0.0  0.0  0.211538  ...   \n",
      "1  0.056818  0.000000  0.75  0.8  0.0  1.0  1.0  0.0  0.0  0.211538  ...   \n",
      "2  0.681818  0.230769  0.25  0.4  0.6  1.0  1.0  0.0  0.0  0.856838  ...   \n",
      "3  0.522727  0.153846  0.00  0.8  0.0  1.0  0.0  1.0  0.0  0.202991  ...   \n",
      "4  0.556818  0.230769  0.25  0.4  0.0  1.0  1.0  0.0  0.0  0.179487  ...   \n",
      "\n",
      "       hemo       pcv        wc        rc  htn        dm  cad  appet   pe  ane  \n",
      "0  0.836735  0.777778  0.231405  0.525424  1.0  1.000000  0.5    0.0  0.0  0.0  \n",
      "1  0.557823  0.644444  0.157025  0.457627  0.0  0.666667  0.5    0.0  0.0  0.0  \n",
      "2  0.442177  0.488889  0.219008  0.457627  0.0  1.000000  0.5    1.0  0.0  1.0  \n",
      "3  0.551020  0.511111  0.185950  0.305085  1.0  0.666667  0.5    1.0  1.0  1.0  \n",
      "4  0.578231  0.577778  0.210744  0.423729  0.0  0.666667  0.5    0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Processed Target (y) head:\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: classification, dtype: int64\n",
      "\n",
      "Shape of X_processed: (398, 24)\n",
      "Shape of y: (398,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Load and Clean Data ---\n",
    "\n",
    "# Load the raw dataset\n",
    "try:\n",
    "    df = pd.read_csv('kidney_disease.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'kidney_disease.csv' not found. Please make sure it's in the same directory.\")\n",
    "    # As a fallback for demonstration, I will load the first few rows you provided.\n",
    "    # In your real run, this 'except' block shouldn't be needed.\n",
    "    from io import StringIO\n",
    "    fallback_data = \"\"\"id,age,bp,sg,al,su,rbc,pc,pcc,ba,bgr,bu,sc,sod,pot,hemo,pcv,wc,rc,htn,dm,cad,appet,pe,ane,classification\n",
    "0,48,80,1.02,1,0,,normal,notpresent,notpresent,121,36,1.2,,,15.4,44,7800,5.2,yes,yes,no,good,no,no,ckd\n",
    "1,7,50,1.02,4,0,,normal,notpresent,notpresent,,18,0.8,,,11.3,38,6000,,no,no,no,good,no,no,ckd\n",
    "2,62,80,1.01,2,3,normal,normal,notpresent,notpresent,423,53,1.8,,,9.6,31,7500,,no,yes,no,poor,no,yes,ckd\n",
    "3,48,70,1.005,4,0,normal,abnormal,present,notpresent,117,56,3.8,111,2.5,11.2,32,6700,3.9,yes,no,no,poor,yes,yes,ckd\n",
    "4,51,80,1.01,2,0,normal,normal,notpresent,notpresent,106,26,1.4,,,11.6,35,7200,4.6,no,no,no,good,no,no,ckd\n",
    "\"\"\"\n",
    "    print(\"Loading fallback data as 'kidney_disease.csv' was not found.\")\n",
    "    df = pd.read_csv(StringIO(fallback_data))\n",
    "\n",
    "\n",
    "print(f\"Original shape of the data: {df.shape}\")\n",
    "\n",
    "# Drop the 'id' column as it's just an identifier\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop('id', axis=1)\n",
    "\n",
    "# Replace special characters and incorrect values with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.replace('\\t?', np.nan, inplace=True) # Handle specific cases if any\n",
    "df.replace('\\t', np.nan, inplace=True)  # Handle tabs if they exist as values\n",
    "\n",
    "# --- 2. Correct Data Types ---\n",
    "\n",
    "# List of columns that should be numeric\n",
    "numeric_cols = ['age', 'bp', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        # 'coerce' will turn any non-numeric values (like '?') into NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# --- 3. Impute Missing Values ---\n",
    "\n",
    "# Separate into numerical and categorical columns for imputation\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Remove the target variable from this list\n",
    "if 'classification' in categorical_cols:\n",
    "    categorical_cols.remove('classification')\n",
    "\n",
    "numerical_cols_with_data = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Impute numerical columns with MEDIAN\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols_with_data] = num_imputer.fit_transform(df[numerical_cols_with_data])\n",
    "\n",
    "# Impute categorical columns with MODE (most frequent)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "print(\"\\n--- Missing values after imputation ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# --- 4. Encode Categorical Data ---\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode all categorical features\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# --- 5. Encode Target Variable ---\n",
    "\n",
    "# Handle the target variable 'classification'\n",
    "# The paper likely maps 'ckd' to 1 and 'notckd' to 0\n",
    "df['classification'] = df['classification'].map({'ckd': 1, 'notckd': 0})\n",
    "# Handle any potential variations like whitespace\n",
    "df['classification'] = df['classification'].replace(r'\\s*ckd\\s*', 1, regex=True)\n",
    "df['classification'] = df['classification'].replace(r'\\s*notckd\\s*', 0, regex=True)\n",
    "\n",
    "# Drop any rows where 'classification' might still be NaN (e.g., if original file had bad data)\n",
    "df.dropna(subset=['classification'], inplace=True)\n",
    "df['classification'] = df['classification'].astype(int)\n",
    "\n",
    "print(\"\\n--- Target variable 'classification' counts ---\")\n",
    "print(df['classification'].value_counts())\n",
    "\n",
    "# --- 6. Separate Features (X) and Target (y) ---\n",
    "X = df.drop('classification', axis=1)\n",
    "y = df['classification']\n",
    "\n",
    "# --- 7. Feature Scaling ---\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to a DataFrame for easier handling in the next stage\n",
    "X_processed = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"\\n--- Preprocessing Complete ---\")\n",
    "print(\"Processed Features (X_processed) head:\")\n",
    "print(X_processed.head())\n",
    "print(\"\\nProcessed Target (y) head:\")\n",
    "print(y.head())\n",
    "print(f\"\\nShape of X_processed: {X_processed.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Load and Clean Data ---\n",
    "\n",
    "# Load the raw dataset\n",
    "try:\n",
    "    df = pd.read_csv('kidney_disease.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'kidney_disease.csv' not found. Please make sure it's in the same directory.\")\n",
    "    # As a fallback for demonstration, I will load the first few rows you provided.\n",
    "    # In your real run, this 'except' block shouldn't be needed.\n",
    "    from io import StringIO\n",
    "    fallback_data = \"\"\"id,age,bp,sg,al,su,rbc,pc,pcc,ba,bgr,bu,sc,sod,pot,hemo,pcv,wc,rc,htn,dm,cad,appet,pe,ane,classification\n",
    "0,48,80,1.02,1,0,,normal,notpresent,notpresent,121,36,1.2,,,15.4,44,7800,5.2,yes,yes,no,good,no,no,ckd\n",
    "1,7,50,1.02,4,0,,normal,notpresent,notpresent,,18,0.8,,,11.3,38,6000,,no,no,no,good,no,no,ckd\n",
    "2,62,80,1.01,2,3,normal,normal,notpresent,notpresent,423,53,1.8,,,9.6,31,7500,,no,yes,no,poor,no,yes,ckd\n",
    "3,48,70,1.005,4,0,normal,abnormal,present,notpresent,117,56,3.8,111,2.5,11.2,32,6700,3.9,yes,no,no,poor,yes,yes,ckd\n",
    "4,51,80,1.01,2,0,normal,normal,notpresent,notpresent,106,26,1.4,,,11.6,35,7200,4.6,no,no,no,good,no,no,ckd\n",
    "\"\"\"\n",
    "    print(\"Loading fallback data as 'kidney_disease.csv' was not found.\")\n",
    "    df = pd.read_csv(StringIO(fallback_data))\n",
    "\n",
    "\n",
    "print(f\"Original shape of the data: {df.shape}\")\n",
    "\n",
    "# Drop the 'id' column as it's just an identifier\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop('id', axis=1)\n",
    "\n",
    "# Replace special characters and incorrect values with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.replace('\\t?', np.nan, inplace=True) # Handle specific cases if any\n",
    "df.replace('\\t', np.nan, inplace=True)  # Handle tabs if they exist as values\n",
    "\n",
    "# --- 2. Correct Data Types ---\n",
    "\n",
    "# List of columns that should be numeric\n",
    "numeric_cols = ['age', 'bp', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        # 'coerce' will turn any non-numeric values (like '?') into NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# --- 3. Impute Missing Values ---\n",
    "\n",
    "# Separate into numerical and categorical columns for imputation\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Remove the target variable from this list\n",
    "if 'classification' in categorical_cols:\n",
    "    categorical_cols.remove('classification')\n",
    "\n",
    "numerical_cols_with_data = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Impute numerical columns with MEDIAN\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols_with_data] = num_imputer.fit_transform(df[numerical_cols_with_data])\n",
    "\n",
    "# Impute categorical columns with MODE (most frequent)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "print(\"\\n--- Missing values after imputation ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# --- 4. Encode Categorical Data ---\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode all categorical features\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# --- 5. Encode Target Variable ---\n",
    "\n",
    "# Handle the target variable 'classification'\n",
    "# The paper likely maps 'ckd' to 1 and 'notckd' to 0\n",
    "df['classification'] = df['classification'].map({'ckd': 1, 'notckd': 0})\n",
    "# Handle any potential variations like whitespace\n",
    "df['classification'] = df['classification'].replace(r'\\s*ckd\\s*', 1, regex=True)\n",
    "df['classification'] = df['classification'].replace(r'\\s*notckd\\s*', 0, regex=True)\n",
    "\n",
    "# Drop any rows where 'classification' might still be NaN (e.g., if original file had bad data)\n",
    "df.dropna(subset=['classification'], inplace=True)\n",
    "df['classification'] = df['classification'].astype(int)\n",
    "\n",
    "print(\"\\n--- Target variable 'classification' counts ---\")\n",
    "print(df['classification'].value_counts())\n",
    "\n",
    "# --- 6. Separate Features (X) and Target (y) ---\n",
    "X = df.drop('classification', axis=1)\n",
    "y = df['classification']\n",
    "\n",
    "# --- 7. Feature Scaling ---\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to a DataFrame for easier handling in the next stage\n",
    "X_processed = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"\\n--- Preprocessing Complete ---\")\n",
    "print(\"Processed Features (X_processed) head:\")\n",
    "print(X_processed.head())\n",
    "print(\"\\nProcessed Target (y) head:\")\n",
    "print(y.head())\n",
    "print(f\"\\nShape of X_processed: {X_processed.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# You now have two crucial variables for the next steps:\n",
    "# 1. X_processed (your clean, scaled features)\n",
    "# 2. y (your clean, encoded target variable)\n",
    "\n",
    "# You now have two crucial variables for the next steps:\n",
    "# 1. X_processed (your clean, scaled features)\n",
    "# 2. y (your clean, encoded target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389c57b-481c-4d79-b40e-91c65ac6aa80",
   "metadata": {},
   "source": [
    "Stage 2: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638715db-e05f-4b4e-8939-e30c320cc08b",
   "metadata": {},
   "source": [
    "1. Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5692daaf-343d-4e64-aea3-0d936245bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_processed and y are already in memory. Proceeding...\n",
      "\n",
      "--- 1. Running Pearson Correlation ---\n",
      "Selected 22 features: ['hemo', 'pcv', 'sg', 'htn', 'rc', 'al', 'dm', 'appet', 'bgr', 'pe', 'pc', 'bu', 'sod', 'ane', 'bp', 'su', 'sc', 'rbc', 'pcc', 'cad', 'age', 'ba']\n",
      "Saved Dataset_1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# --- Re-create X_processed and y if not in memory ---\n",
    "# (You can skip this part if X_processed and y are already in your environment)\n",
    "try:\n",
    "    # This is to make sure X_processed and y exist\n",
    "    X_processed.shape\n",
    "    y.shape\n",
    "    print(\"X_processed and y are already in memory. Proceeding...\")\n",
    "except NameError:\n",
    "    print(\"X_processed and y not found in memory. Re-running essential parts of Stage 1...\")\n",
    "    # A minimal re-run to get X_processed and y\n",
    "    df = pd.read_csv('kidney_disease.csv')\n",
    "    df = df.drop('id', axis=1)\n",
    "    df.replace(['?', '\\t?', '\\t'], np.nan, inplace=True)\n",
    "    \n",
    "    numeric_cols = ['age', 'bp', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols.remove('classification')\n",
    "    numerical_cols_with_data = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    from sklearn.impute import SimpleImputer\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    df[numerical_cols_with_data] = num_imputer.fit_transform(df[numerical_cols_with_data])\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        \n",
    "    df['classification'] = df['classification'].map({'ckd': 1, 'notckd': 0})\n",
    "    df['classification'] = df['classification'].replace(r'\\s*ckd\\s*', 1, regex=True)\n",
    "    df['classification'] = df['classification'].replace(r'\\s*notckd\\s*', 0, regex=True)\n",
    "    df.dropna(subset=['classification'], inplace=True)\n",
    "    df['classification'] = df['classification'].astype(int)\n",
    "    \n",
    "    X = df.drop('classification', axis=1)\n",
    "    y = df['classification']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_processed = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    print(\"Re-creation complete.\")\n",
    "# --- End of re-creation block ---\n",
    "\n",
    "\n",
    "# 1. Pearson Correlation\n",
    "print(\"\\n--- 1. Running Pearson Correlation ---\")\n",
    "# Calculate Pearson correlation\n",
    "cor = X_processed.corrwith(y)\n",
    "cor_target = abs(cor)\n",
    "\n",
    "# Select the top 22 features\n",
    "relevant_features_pearson = cor_target.nlargest(22).index\n",
    "\n",
    "# Create the new dataset\n",
    "X_pearson = X_processed[relevant_features_pearson]\n",
    "Dataset_1 = X_pearson.copy()\n",
    "Dataset_1['label'] = y.values\n",
    "\n",
    "print(f\"Selected {X_pearson.shape[1]} features: {list(relevant_features_pearson)}\")\n",
    "Dataset_1.to_csv('Dataset_1.csv', index=False)\n",
    "print(\"Saved Dataset_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a467c45-90e8-4bcf-bf1a-671b087d45fe",
   "metadata": {},
   "source": [
    "2. CHI-SQUARE (SelectKBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c03b12a-dfb5-4304-9ce4-bc7a3e7475f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Running Chi-Square (SelectKBest) ---\n",
      "Selected 18 features: ['sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr', 'bu', 'sc', 'hemo', 'pcv', 'rc', 'htn', 'dm', 'appet', 'pe', 'ane']\n",
      "Saved Dataset_2.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# 2. CHI-SQUARE\n",
    "print(\"\\n--- 2. Running Chi-Square (SelectKBest) ---\")\n",
    "# Select top 18 features\n",
    "chi_selector = SelectKBest(chi2, k=18)\n",
    "X_chi2_arr = chi_selector.fit_transform(X_processed, y)\n",
    "relevant_features_chi2 = X_processed.columns[chi_selector.get_support()]\n",
    "\n",
    "# Create the new dataset\n",
    "X_chi2 = pd.DataFrame(X_chi2_arr, columns=relevant_features_chi2)\n",
    "Dataset_2 = X_chi2.copy()\n",
    "Dataset_2['label'] = y.values\n",
    "\n",
    "print(f\"Selected {X_chi2.shape[1]} features: {list(relevant_features_chi2)}\")\n",
    "Dataset_2.to_csv('Dataset_2.csv', index=False)\n",
    "print(\"Saved Dataset_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0f79d-05bc-4b11-bc03-4eb7dad519d9",
   "metadata": {},
   "source": [
    "3. VARIANCE THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06605f2-b868-4a80-aa30-4ed353140b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Running Variance Threshold ---\n",
      "Selected 7 features: ['htn', 'appet', 'pe', 'pc', 'ane', 'rbc', 'pcc']\n",
      "Saved Dataset_3.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# 3. VARIANCE THRESHOLD\n",
    "print(\"\\n--- 3. Running Variance Threshold ---\")\n",
    "# Calculate variances\n",
    "variances = X_processed.var()\n",
    "\n",
    "# Get the names of the top 7 features with the highest variance\n",
    "relevant_features_var = variances.nlargest(7).index\n",
    "\n",
    "# Create the new dataset\n",
    "X_var_thresh = X_processed[relevant_features_var]\n",
    "Dataset_3 = X_var_thresh.copy()\n",
    "Dataset_3['label'] = y.values\n",
    "\n",
    "print(f\"Selected {X_var_thresh.shape[1]} features: {list(relevant_features_var)}\")\n",
    "Dataset_3.to_csv('Dataset_3.csv', index=False)\n",
    "print(\"Saved Dataset_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d8537-4ae7-4254-ab76-408dc93cadab",
   "metadata": {},
   "source": [
    "4. RECURSIVE FEATURE ELIMINATION (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6509b1-faa7-4c4e-992b-0c90376e0251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Running RFE ---\n",
      "Selected 11 features: ['sg', 'al', 'su', 'pc', 'hemo', 'pcv', 'rc', 'htn', 'dm', 'appet', 'pe']\n",
      "Saved Dataset_4.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 4. RFE (Recursive Feature Elimination)\n",
    "print(\"\\n--- 4. Running RFE ---\")\n",
    "# Initialize the model and RFE\n",
    "# We use max_iter=1000 to ensure convergence\n",
    "model_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(estimator=model_lr, n_features_to_select=11)\n",
    "\n",
    "# Fit RFE\n",
    "X_rfe_arr = rfe.fit_transform(X_processed, y)\n",
    "relevant_features_rfe = X_processed.columns[rfe.get_support()]\n",
    "\n",
    "# Create the new dataset\n",
    "X_rfe = pd.DataFrame(X_rfe_arr, columns=relevant_features_rfe)\n",
    "Dataset_4 = X_rfe.copy()\n",
    "Dataset_4['label'] = y.values\n",
    "\n",
    "print(f\"Selected {X_rfe.shape[1]} features: {list(relevant_features_rfe)}\")\n",
    "Dataset_4.to_csv('Dataset_4.csv', index=False)\n",
    "print(\"Saved Dataset_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5c1b9-b717-44b9-b0c0-402a8a53d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. SEQUENTIAL FEATURE SELECTION (SFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbac83a0-38f1-45d9-8d97-afd080856fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Running SFS ---\n",
      "Selected 11 features: ['bp', 'sg', 'al', 'su', 'bu', 'hemo', 'pcv', 'rc', 'htn', 'dm', 'pe']\n",
      "Saved Dataset_5.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 5. SFS (Sequential Feature Selection)\n",
    "print(\"\\n--- 5. Running SFS ---\")\n",
    "# Note: SFS can be slow. n_jobs=-1 uses all available CPU cores.\n",
    "model_lr_sfs = LogisticRegression(max_iter=1000, random_state=42)\n",
    "sfs = SequentialFeatureSelector(estimator=model_lr_sfs, \n",
    "                              n_features_to_select=11, \n",
    "                              direction='forward', \n",
    "                              cv=5, \n",
    "                              n_jobs=-1)\n",
    "# Fit SFS\n",
    "X_sfs_arr = sfs.fit_transform(X_processed, y)\n",
    "relevant_features_sfs = X_processed.columns[sfs.get_support()]\n",
    "\n",
    "# Create the new dataset\n",
    "X_sfs = pd.DataFrame(X_sfs_arr, columns=relevant_features_sfs)\n",
    "Dataset_5 = X_sfs.copy()\n",
    "Dataset_5['label'] = y.values\n",
    "\n",
    "print(f\"Selected {X_sfs.shape[1]} features: {list(relevant_features_sfs)}\")\n",
    "Dataset_5.to_csv('Dataset_5.csv', index=False)\n",
    "print(\"Saved Dataset_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5ca75-c8b5-4c57-a790-06cf431b5bd5",
   "metadata": {},
   "source": [
    "6. LASSO REGRESSION (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b519ca29-30a2-48be-9aee-36e9f4018b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Running Lasso Regression ---\n",
      "Selected 16 features: ['bp', 'sg', 'al', 'rbc', 'pc', 'pcc', 'ba', 'bgr', 'bu', 'hemo', 'pcv', 'htn', 'dm', 'appet', 'pe', 'ane']\n",
      "Saved Dataset_6.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# 6. LASSO REGRESSION\n",
    "print(\"\\n--- 6. Running Lasso Regression ---\")\n",
    "# Use LassoCV to find the best alpha (regularization strength)\n",
    "# We set a high max_iter for convergence\n",
    "lasso_cv = LassoCV(cv=5, max_iter=10000, random_state=42)\n",
    "lasso_cv.fit(X_processed, y)\n",
    "\n",
    "# SelectFromModel will pick features whose coefficients are non-zero\n",
    "sfm_lasso = SelectFromModel(lasso_cv, threshold=1e-5) # Use a small threshold\n",
    "sfm_lasso.fit(X_processed, y)\n",
    "\n",
    "relevant_features_lasso = X_processed.columns[sfm_lasso.get_support()]\n",
    "\n",
    "# Create the new dataset\n",
    "X_lasso = X_processed[relevant_features_lasso]\n",
    "Dataset_6 = X_lasso.copy()\n",
    "Dataset_6['label'] = y.values\n",
    "\n",
    "print(f\"Selected {X_lasso.shape[1]} features: {list(relevant_features_lasso)}\")\n",
    "Dataset_6.to_csv('Dataset_6.csv', index=False)\n",
    "print(\"Saved Dataset_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd3834-e252-4d45-a194-7751e933cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. RIDGE REGRESSION (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf74883-7824-4d1e-bc20-6f290edfa327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Running Ridge Regression (with RFE) ---\n",
      "Selected 11 features: ['bp', 'sg', 'al', 'bgr', 'bu', 'sc', 'sod', 'hemo', 'htn', 'dm', 'cad']\n",
      "Saved Dataset_7.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# 7. RIDGE REGRESSION (used with RFE)\n",
    "print(\"\\n--- 7. Running Ridge Regression (with RFE) ---\")\n",
    "# Use RidgeCV to find the best alpha during cross-validation\n",
    "ridge_cv = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=5)\n",
    "\n",
    "# Use RFE to select the 11 most important features as ranked by Ridge\n",
    "rfe_ridge = RFE(estimator=ridge_cv, n_features_to_select=11)\n",
    "X_ridge_arr = rfe_ridge.fit_transform(X_processed, y)\n",
    "relevant_features_ridge = X_processed.columns[rfe_ridge.get_support()]\n",
    "\n",
    "# Create the new dataset\n",
    "X_ridge = pd.DataFrame(X_ridge_arr, columns=relevant_features_ridge)\n",
    "Dataset_7 = X_ridge.copy()\n",
    "Dataset_7['label'] = y.values\n",
    "\n",
    "print(f\"Selected {X_ridge.shape[1]} features: {list(relevant_features_ridge)}\")\n",
    "Dataset_7.to_csv('Dataset_7.csv', index=False)\n",
    "print(\"Saved Dataset_7.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43fa73e-3a9f-4201-9371-cf22e4b6af09",
   "metadata": {},
   "source": [
    "Stage 3 (Part A): K-Fold Classification & Classifier 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1060df2-4604-45af-a819-e8aad32a4633",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84ef771-0a43-4931-a9ac-fc494b4b9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset_1.csv with 22 features.\n",
      "Loaded Dataset_2.csv with 18 features.\n",
      "Loaded Dataset_3.csv with 7 features.\n",
      "Loaded Dataset_4.csv with 11 features.\n",
      "Loaded Dataset_5.csv with 11 features.\n",
      "Loaded Dataset_6.csv with 16 features.\n",
      "Loaded Dataset_7.csv with 11 features.\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING CLASSIFIER 1: RANDOM FOREST {'=' * 20}\n",
      "\n",
      "--- Evaluating Random Forest on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0161)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9950 (± 0.0218)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9923 (± 0.0304)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Random Forest on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0161)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0238)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9923 (± 0.0304)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Random Forest on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8996 (± 0.0446)\n",
      "    AUC Score: 0.9199 (± 0.0341)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8995 (± 0.0613)\n",
      "    AUC Score: 0.9204 (± 0.0474)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9002 (± 0.0974)\n",
      "    AUC Score: 0.9206 (± 0.0774)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Random Forest on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9975 (± 0.0075)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9975 (± 0.0109)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9949 (± 0.0192)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Random Forest on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0161)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0238)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9923 (± 0.0304)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Random Forest on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9899 (± 0.0166)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9900 (± 0.0200)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9923 (± 0.0231)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Random Forest on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0161)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9900 (± 0.0300)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9974 (± 0.0138)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED CLASSIFIER 1: RANDOM FOREST {'=' * 20}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import all classifiers we will use\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Load All 7 Datasets ---\n",
    "\n",
    "datasets = {}\n",
    "for i in range(1, 8):\n",
    "    try:\n",
    "        filename = f'Dataset_{i}.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "        datasets[filename] = {\n",
    "            'X': df.drop('label', axis=1),\n",
    "            'y': df['label']\n",
    "        }\n",
    "        print(f\"Loaded {filename} with {datasets[filename]['X'].shape[1]} features.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found.\")\n",
    "\n",
    "# --- 2. Define the K-Fold Evaluation Function ---\n",
    "\n",
    "def evaluate_model_kfold(model, model_name, dataset_name, X, y):\n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation for k=10, 20, 30 and prints results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluating {model_name} on {dataset_name} ---\")\n",
    "    \n",
    "    # We use a pipeline for consistency, especially for models like SVM\n",
    "    # that are sensitive to scaling (even though our data is already scaled).\n",
    "    # For SVC, we need probability=True to calculate AUC.\n",
    "    if model_name == \"Support Vector Machine\":\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', SVC(probability=True, random_state=42))\n",
    "        ])\n",
    "    else:\n",
    "         pipeline = Pipeline([\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    # Define the k-values\n",
    "    k_values = [10, 20, 30]\n",
    "    \n",
    "    for k in k_values:\n",
    "        try:\n",
    "            # Set up the K-Fold\n",
    "            kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "            \n",
    "            # Get cross-validation scores for accuracy\n",
    "            acc_scores = cross_val_score(pipeline, X, y, cv=kfold, scoring='accuracy')\n",
    "            \n",
    "            # Get cross-validation scores for AUC\n",
    "            # Some models/data might struggle with AUC in some folds\n",
    "            try:\n",
    "                auc_scores = cross_val_score(pipeline, X, y, cv=kfold, scoring='roc_auc')\n",
    "                auc_mean = np.mean(auc_scores)\n",
    "            except ValueError as e:\n",
    "                # print(f\"  Warning (k={k}): Could not calculate AUC. {e}\")\n",
    "                auc_mean = np.nan # Report as Not a Number if it fails\n",
    "            \n",
    "            print(f\"  K = {k}:\")\n",
    "            print(f\"    Accuracy: {np.mean(acc_scores):.4f} (± {np.std(acc_scores):.4f})\")\n",
    "            print(f\"    AUC Score: {auc_mean:.4f} (± {np.std(auc_scores):.4f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error during k={k} evaluation: {e}\")\n",
    "            \n",
    "    print(\"-\" * (20 + len(model_name) + len(dataset_name)))\n",
    "\n",
    "\n",
    "# --- 3. Run Classifier 1: Random Forest ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING CLASSIFIER 1: RANDOM FOREST {'=' * 20}\")\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the model\n",
    "for name, data in datasets.items():\n",
    "    evaluate_model_kfold(rf_model, \"Random Forest\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED CLASSIFIER 1: RANDOM FOREST {'=' * 20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90788d-b7c0-4d33-b413-a6adbb7cdd0e",
   "metadata": {},
   "source": [
    "Gradient Boosting, AdaBoost, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6197b61a-1cc0-40da-a351-ee3803e940f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies from Stage 3 (Part A) found. Proceeding...\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING CLASSIFIER 2: GRADIENT BOOSTING {'=' * 20}\n",
      "\n",
      "--- Evaluating Gradient Boosting on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9874 (± 0.0168)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9875 (± 0.0268)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9850 (± 0.0360)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Evaluating Gradient Boosting on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9899 (± 0.0123)\n",
      "    AUC Score: 0.9995 (± 0.0016)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0179)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9899 (± 0.0257)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Evaluating Gradient Boosting on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8996 (± 0.0446)\n",
      "    AUC Score: 0.9199 (± 0.0341)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8995 (± 0.0613)\n",
      "    AUC Score: 0.9204 (± 0.0474)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9002 (± 0.0974)\n",
      "    AUC Score: 0.9206 (± 0.0774)\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Evaluating Gradient Boosting on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9899 (± 0.0169)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9900 (± 0.0255)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9897 (± 0.0328)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Evaluating Gradient Boosting on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0163)\n",
      "    AUC Score: 0.9989 (± 0.0024)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0179)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9923 (± 0.0231)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Evaluating Gradient Boosting on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9899 (± 0.0169)\n",
      "    AUC Score: 0.9992 (± 0.0024)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9950 (± 0.0150)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9923 (± 0.0231)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Evaluating Gradient Boosting on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9874 (± 0.0168)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9900 (± 0.0255)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9874 (± 0.0346)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "--------------------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED CLASSIFIER 2: GRADIENT BOOSTING {'=' * 20}\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING CLASSIFIER 3: ADABOOST {'=' * 20}\n",
      "\n",
      "--- Evaluating AdaBoost on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0116)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0179)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9925 (± 0.0225)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-----------------------------------------\n",
      "\n",
      "--- Evaluating AdaBoost on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0116)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9875 (± 0.0217)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9925 (± 0.0225)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-----------------------------------------\n",
      "\n",
      "--- Evaluating AdaBoost on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8845 (± 0.0420)\n",
      "    AUC Score: 0.9161 (± 0.0336)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8870 (± 0.0649)\n",
      "    AUC Score: 0.9156 (± 0.0468)\n",
      "  K = 30:\n",
      "    Accuracy: 0.8822 (± 0.0945)\n",
      "    AUC Score: 0.9154 (± 0.0760)\n",
      "-----------------------------------------\n",
      "\n",
      "--- Evaluating AdaBoost on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9823 (± 0.0256)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9825 (± 0.0396)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9822 (± 0.0511)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "-----------------------------------------\n",
      "\n",
      "--- Evaluating AdaBoost on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9848 (± 0.0260)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9875 (± 0.0349)\n",
      "    AUC Score: 0.9998 (± 0.0011)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9897 (± 0.0432)\n",
      "    AUC Score: 0.9996 (± 0.0021)\n",
      "-----------------------------------------\n",
      "\n",
      "--- Evaluating AdaBoost on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9874 (± 0.0171)\n",
      "    AUC Score: 0.9992 (± 0.0024)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9850 (± 0.0357)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9874 (± 0.0445)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-----------------------------------------\n",
      "\n",
      "--- Evaluating AdaBoost on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9949 (± 0.0101)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0179)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9901 (± 0.0252)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-----------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED CLASSIFIER 3: ADABOOST {'=' * 20}\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING CLASSIFIER 4: XGBOOST {'=' * 20}\n",
      "\n",
      "--- Evaluating XGBoost on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9874 (± 0.0204)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9875 (± 0.0268)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9848 (± 0.0363)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "----------------------------------------\n",
      "\n",
      "--- Evaluating XGBoost on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9823 (± 0.0230)\n",
      "    AUC Score: 0.9995 (± 0.0016)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9875 (± 0.0217)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9848 (± 0.0363)\n",
      "    AUC Score: 0.9992 (± 0.0045)\n",
      "----------------------------------------\n",
      "\n",
      "--- Evaluating XGBoost on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8946 (± 0.0443)\n",
      "    AUC Score: 0.9199 (± 0.0341)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8945 (± 0.0589)\n",
      "    AUC Score: 0.9204 (± 0.0474)\n",
      "  K = 30:\n",
      "    Accuracy: 0.8951 (± 0.0961)\n",
      "    AUC Score: 0.9206 (± 0.0774)\n",
      "----------------------------------------\n",
      "\n",
      "--- Evaluating XGBoost on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9874 (± 0.0204)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9900 (± 0.0255)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9846 (± 0.0502)\n",
      "    AUC Score: 0.9984 (± 0.0085)\n",
      "----------------------------------------\n",
      "\n",
      "--- Evaluating XGBoost on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9823 (± 0.0256)\n",
      "    AUC Score: 0.9989 (± 0.0032)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9825 (± 0.0396)\n",
      "    AUC Score: 0.9990 (± 0.0044)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9846 (± 0.0502)\n",
      "    AUC Score: 0.9984 (± 0.0085)\n",
      "----------------------------------------\n",
      "\n",
      "--- Evaluating XGBoost on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9849 (± 0.0202)\n",
      "    AUC Score: 0.9984 (± 0.0032)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9850 (± 0.0278)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9846 (± 0.0366)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "----------------------------------------\n",
      "\n",
      "--- Evaluating XGBoost on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9874 (± 0.0171)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9875 (± 0.0217)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9875 (± 0.0279)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "----------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED CLASSIFIER 4: XGBOOST {'=' * 20}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "# Import the classifiers for this stage\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC # Need this for the evaluation function logic\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# --- Check if datasets and function exist from previous step ---\n",
    "try:\n",
    "    datasets\n",
    "    evaluate_model_kfold\n",
    "    print(\"Dependencies from Stage 3 (Part A) found. Proceeding...\")\n",
    "except NameError:\n",
    "    print(\"Error: 'datasets' or 'evaluate_model_kfold' not found.\")\n",
    "    print(\"Please re-run the code from Stage 3 (Part A) first.\")\n",
    "    # Stop execution if dependencies are missing\n",
    "    raise\n",
    "\n",
    "# --- 4. Run Classifier 2: Gradient Boosting ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING CLASSIFIER 2: GRADIENT BOOSTING {'=' * 20}\")\n",
    "\n",
    "# Initialize the model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the model\n",
    "for name, data in datasets.items():\n",
    "    evaluate_model_kfold(gb_model, \"Gradient Boosting\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED CLASSIFIER 2: GRADIENT BOOSTING {'=' * 20}\")\n",
    "\n",
    "\n",
    "# --- 5. Run Classifier 3: AdaBoost (Adaptive Boosting) ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING CLASSIFIER 3: ADABOOST {'=' * 20}\")\n",
    "\n",
    "# Initialize the model\n",
    "# AdaBoost often works well with a simple Decision Tree as its base estimator\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ada_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), random_state=42)\n",
    "\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the model\n",
    "for name, data in datasets.items():\n",
    "    evaluate_model_kfold(ada_model, \"AdaBoost\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED CLASSIFIER 3: ADABOOST {'=' * 20}\")\n",
    "\n",
    "\n",
    "# --- 6. Run Classifier 4: XGBoost (eXtreme Gradient Boosting) ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING CLASSIFIER 4: XGBOOST {'=' * 20}\")\n",
    "\n",
    "# Initialize the model\n",
    "# eval_metric='logloss' is common for binary classification\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the model\n",
    "for name, data in datasets.items():\n",
    "    evaluate_model_kfold(xgb_model, \"XGBoost\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED CLASSIFIER 4: XGBOOST {'=' * 20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ad903-d772-4931-9862-32a88925c28c",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM), Decision Tree, and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6a47c6-2968-4447-a353-6ae8218ca680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies from Stage 3 (Part A) found. Proceeding...\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING CLASSIFIER 5: SUPPORT VECTOR MACHINE {'=' * 20}\n",
      "\n",
      "--- Evaluating Support Vector Machine on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9950 (± 0.0100)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9950 (± 0.0150)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9951 (± 0.0185)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-------------------------------------------------------\n",
      "\n",
      "--- Evaluating Support Vector Machine on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9975 (± 0.0075)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9975 (± 0.0109)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9976 (± 0.0128)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-------------------------------------------------------\n",
      "\n",
      "--- Evaluating Support Vector Machine on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8996 (± 0.0446)\n",
      "    AUC Score: 0.9199 (± 0.0341)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8995 (± 0.0613)\n",
      "    AUC Score: 0.9204 (± 0.0474)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9002 (± 0.0974)\n",
      "    AUC Score: 0.9206 (± 0.0774)\n",
      "-------------------------------------------------------\n",
      "\n",
      "--- Evaluating Support Vector Machine on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9950 (± 0.0150)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9975 (± 0.0109)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9976 (± 0.0128)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-------------------------------------------------------\n",
      "\n",
      "--- Evaluating Support Vector Machine on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9874 (± 0.0258)\n",
      "    AUC Score: 0.9995 (± 0.0016)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9900 (± 0.0255)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9901 (± 0.0321)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "-------------------------------------------------------\n",
      "\n",
      "--- Evaluating Support Vector Machine on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9925 (± 0.0225)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0238)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9929 (± 0.0283)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "-------------------------------------------------------\n",
      "\n",
      "--- Evaluating Support Vector Machine on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9799 (± 0.0220)\n",
      "    AUC Score: 0.9987 (± 0.0039)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9800 (± 0.0292)\n",
      "    AUC Score: 0.9980 (± 0.0087)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9802 (± 0.0384)\n",
      "    AUC Score: 0.9984 (± 0.0085)\n",
      "-------------------------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED CLASSIFIER 5: SUPPORT VECTOR MACHINE {'=' * 20}\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING CLASSIFIER 6: DECISION TREE {'=' * 20}\n",
      "\n",
      "--- Evaluating Decision Tree on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9699 (± 0.0270)\n",
      "    AUC Score: 0.9697 (± 0.0293)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9700 (± 0.0332)\n",
      "    AUC Score: 0.9693 (± 0.0381)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9700 (± 0.0457)\n",
      "    AUC Score: 0.9677 (± 0.0656)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Decision Tree on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9773 (± 0.0265)\n",
      "    AUC Score: 0.9786 (± 0.0252)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9800 (± 0.0332)\n",
      "    AUC Score: 0.9806 (± 0.0321)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9775 (± 0.0481)\n",
      "    AUC Score: 0.9787 (± 0.0470)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Decision Tree on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8996 (± 0.0446)\n",
      "    AUC Score: 0.9199 (± 0.0341)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8995 (± 0.0613)\n",
      "    AUC Score: 0.9204 (± 0.0474)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9002 (± 0.0974)\n",
      "    AUC Score: 0.9206 (± 0.0774)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Decision Tree on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9824 (± 0.0227)\n",
      "    AUC Score: 0.9855 (± 0.0192)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9750 (± 0.0371)\n",
      "    AUC Score: 0.9753 (± 0.0383)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9799 (± 0.0430)\n",
      "    AUC Score: 0.9785 (± 0.0476)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Decision Tree on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9824 (± 0.0197)\n",
      "    AUC Score: 0.9831 (± 0.0191)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9800 (± 0.0292)\n",
      "    AUC Score: 0.9811 (± 0.0290)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9800 (± 0.0386)\n",
      "    AUC Score: 0.9798 (± 0.0433)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Decision Tree on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9547 (± 0.0219)\n",
      "    AUC Score: 0.9574 (± 0.0247)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9624 (± 0.0350)\n",
      "    AUC Score: 0.9654 (± 0.0315)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9577 (± 0.0500)\n",
      "    AUC Score: 0.9600 (± 0.0500)\n",
      "----------------------------------------------\n",
      "\n",
      "--- Evaluating Decision Tree on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9774 (± 0.0236)\n",
      "    AUC Score: 0.9777 (± 0.0229)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9774 (± 0.0296)\n",
      "    AUC Score: 0.9758 (± 0.0361)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9749 (± 0.0407)\n",
      "    AUC Score: 0.9629 (± 0.0739)\n",
      "----------------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED CLASSIFIER 6: DECISION TREE {'=' * 20}\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING CLASSIFIER 7: LOGISTIC REGRESSION {'=' * 20}\n",
      "\n",
      "--- Evaluating Logistic Regression on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9824 (± 0.0297)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9799 (± 0.0369)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9830 (± 0.0444)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Evaluating Logistic Regression on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9824 (± 0.0297)\n",
      "    AUC Score: 0.9994 (± 0.0017)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9799 (± 0.0369)\n",
      "    AUC Score: 0.9994 (± 0.0025)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9804 (± 0.0456)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Evaluating Logistic Regression on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8996 (± 0.0446)\n",
      "    AUC Score: 0.9199 (± 0.0341)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8995 (± 0.0613)\n",
      "    AUC Score: 0.9204 (± 0.0474)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9002 (± 0.0974)\n",
      "    AUC Score: 0.9206 (± 0.0774)\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Evaluating Logistic Regression on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9799 (± 0.0292)\n",
      "    AUC Score: 0.9994 (± 0.0017)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9774 (± 0.0371)\n",
      "    AUC Score: 0.9994 (± 0.0025)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9780 (± 0.0464)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Evaluating Logistic Regression on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9823 (± 0.0253)\n",
      "    AUC Score: 0.9995 (± 0.0011)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9824 (± 0.0328)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9850 (± 0.0360)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Evaluating Logistic Regression on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9749 (± 0.0274)\n",
      "    AUC Score: 0.9992 (± 0.0025)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9774 (± 0.0371)\n",
      "    AUC Score: 0.9989 (± 0.0050)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9753 (± 0.0474)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Evaluating Logistic Regression on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9697 (± 0.0190)\n",
      "    AUC Score: 0.9967 (± 0.0060)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9697 (± 0.0293)\n",
      "    AUC Score: 0.9956 (± 0.0130)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9700 (± 0.0418)\n",
      "    AUC Score: 0.9952 (± 0.0152)\n",
      "----------------------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED CLASSIFIER 7: LOGISTIC REGRESSION {'=' * 20}\n",
      "\n",
      "\n",
      "*** All 7 classifiers have been evaluated on all 7 datasets. ***\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "# Import the classifiers for this stage\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import other classifiers needed for the function logic\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# --- Check if datasets and function exist from previous step ---\n",
    "try:\n",
    "    datasets\n",
    "    evaluate_model_kfold\n",
    "    print(\"Dependencies from Stage 3 (Part A) found. Proceeding...\")\n",
    "except NameError:\n",
    "    print(\"Error: 'datasets' or 'evaluate_model_kfold' not found.\")\n",
    "    print(\"Please re-run the code from Stage 3 (Part A) first.\")\n",
    "    # Stop execution if dependencies are missing\n",
    "    raise\n",
    "\n",
    "# --- 7. Run Classifier 5: Support Vector Machine (SVM) ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING CLASSIFIER 5: SUPPORT VECTOR MACHINE {'=' * 20}\")\n",
    "\n",
    "# Initialize the model\n",
    "# We set probability=True to be able to calculate ROC AUC scores\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the model\n",
    "for name, data in datasets.items():\n",
    "    # The 'evaluate_model_kfold' function has special handling\n",
    "    # to put SVM in a pipeline with a StandardScaler,\n",
    "    # as SVMs are very sensitive to feature scales.\n",
    "    evaluate_model_kfold(svm_model, \"Support Vector Machine\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED CLASSIFIER 5: SUPPORT VECTOR MACHINE {'=' * 20}\")\n",
    "\n",
    "\n",
    "# --- 8. Run Classifier 6: Decision Tree ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING CLASSIFIER 6: DECISION TREE {'=' * 20}\")\n",
    "\n",
    "# Initialize the model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the model\n",
    "for name, data in datasets.items():\n",
    "    evaluate_model_kfold(dt_model, \"Decision Tree\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED CLASSIFIER 6: DECISION TREE {'=' * 20}\")\n",
    "\n",
    "\n",
    "# --- 9. Run Classifier 7: Logistic Regression ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING CLASSIFIER 7: LOGISTIC REGRESSION {'=' * 20}\")\n",
    "\n",
    "# Initialize the model\n",
    "# We set max_iter=1000 to ensure convergence\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the model\n",
    "for name, data in datasets.items():\n",
    "    evaluate_model_kfold(lr_model, \"Logistic Regression\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED CLASSIFIER 7: LOGISTIC REGRESSION {'=' * 20}\")\n",
    "\n",
    "print(\"\\n\\n*** All 7 classifiers have been evaluated on all 7 datasets. ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f4e8f-16e5-4bf9-ac70-707a71416768",
   "metadata": {},
   "source": [
    "Stage 4: Model Blending (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33bad993-8f37-4f40-88ee-bc556dcd3a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies from Stage 3 found. Proceeding with Blending...\n",
      "\n",
      "\n",
      "{'=' * 20} STARTING STAGE 4: MODEL BLENDING (STACKING) {'=' * 20}\n",
      "\n",
      "--- Evaluating Blended (Stacking) Model on Dataset_1.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9925 (± 0.0160)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0238)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9925 (± 0.0300)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "---------------------------------------------------------\n",
      "\n",
      "--- Evaluating Blended (Stacking) Model on Dataset_2.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 1.0000 (± 0.0000)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9975 (± 0.0109)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9974 (± 0.0138)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "---------------------------------------------------------\n",
      "\n",
      "--- Evaluating Blended (Stacking) Model on Dataset_3.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.8996 (± 0.0446)\n",
      "    AUC Score: 0.9199 (± 0.0341)\n",
      "  K = 20:\n",
      "    Accuracy: 0.8995 (± 0.0613)\n",
      "    AUC Score: 0.9204 (± 0.0474)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9002 (± 0.0974)\n",
      "    AUC Score: 0.9206 (± 0.0774)\n",
      "---------------------------------------------------------\n",
      "\n",
      "--- Evaluating Blended (Stacking) Model on Dataset_4.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9975 (± 0.0075)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9975 (± 0.0109)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9974 (± 0.0138)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "---------------------------------------------------------\n",
      "\n",
      "--- Evaluating Blended (Stacking) Model on Dataset_5.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0163)\n",
      "    AUC Score: 0.9995 (± 0.0016)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9950 (± 0.0150)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9949 (± 0.0192)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "---------------------------------------------------------\n",
      "\n",
      "--- Evaluating Blended (Stacking) Model on Dataset_6.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9949 (± 0.0103)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9950 (± 0.0150)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9949 (± 0.0192)\n",
      "    AUC Score: 1.0000 (± 0.0000)\n",
      "---------------------------------------------------------\n",
      "\n",
      "--- Evaluating Blended (Stacking) Model on Dataset_7.csv ---\n",
      "  K = 10:\n",
      "    Accuracy: 0.9924 (± 0.0116)\n",
      "    AUC Score: 0.9997 (± 0.0008)\n",
      "  K = 20:\n",
      "    Accuracy: 0.9925 (± 0.0238)\n",
      "    AUC Score: 0.9995 (± 0.0022)\n",
      "  K = 30:\n",
      "    Accuracy: 0.9899 (± 0.0325)\n",
      "    AUC Score: 0.9992 (± 0.0043)\n",
      "---------------------------------------------------------\n",
      "\n",
      "{'=' * 20} COMPLETED STAGE 4: MODEL BLENDING (STACKING) {'=' * 20}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "# --- 1. Import All Classifiers for Base Models ---\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier, \n",
    "    AdaBoostClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 2. Check if datasets and function exist from previous step ---\n",
    "try:\n",
    "    datasets\n",
    "    evaluate_model_kfold\n",
    "    print(\"Dependencies from Stage 3 found. Proceeding with Blending...\")\n",
    "except NameError:\n",
    "    print(\"Error: 'datasets' or 'evaluate_model_kfold' not found.\")\n",
    "    print(\"Please re-run the code from Stage 3 (Part A) first.\")\n",
    "    # Stop execution if dependencies are missing\n",
    "    raise\n",
    "\n",
    "# --- 3. Define the Base Models (Level 0) ---\n",
    "# We use the same settings as before for consistency\n",
    "base_models = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=42)),\n",
    "    ('GradientBoosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('AdaBoost', AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "    ('SVM', SVC(probability=True, random_state=42)),\n",
    "    ('DecisionTree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('LogisticRegression', LogisticRegression(random_state=42, max_iter=1000))\n",
    "]\n",
    "\n",
    "# --- 4. Define the Meta-Model (Level 1) ---\n",
    "# A Logistic Regression is a common and effective meta-model\n",
    "meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# --- 5. Create the Stacking (Blending) Classifier ---\n",
    "# 'passthrough=True' means the meta-model gets both the\n",
    "# original features AND the predictions from the base models.\n",
    "# 'cv=5' means the base models' predictions are generated\n",
    "# using 5-fold cross-validation to prevent data leakage.\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=False, # Set to True if you want meta-model to also see original features\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- 6. Run the Blended Model Evaluation ---\n",
    "\n",
    "print(\"\\n\\n{'=' * 20} STARTING STAGE 4: MODEL BLENDING (STACKING) {'=' * 20}\")\n",
    "\n",
    "# Loop through all 7 datasets and evaluate the blended model\n",
    "for name, data in datasets.items():\n",
    "    evaluate_model_kfold(stacking_model, \"Blended (Stacking) Model\", name, data['X'], data['y'])\n",
    "\n",
    "print(\"\\n{'=' * 20} COMPLETED STAGE 4: MODEL BLENDING (STACKING) {'=' * 20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1d613-0314-4d2e-90f9-568d963317d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
